from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.common.action_chains import ActionChains
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.remote.webelement import WebElement
import pandas as pd
import time, os, json
import re

# ========================
# C·∫•u h√¨nh ng∆∞·ªùi d√πng
# ========================
cr_cb = [
    ('r1', 'b4'), ('r1', 'b5'), ('r1', 'b6'), ('r1', 'b7'),
    ('r1', 'b8'), ('r1', 'b9'), ('r1', 'b10'), ('r1', 'b11'),
    ('r1', 'b12'), ('r1', 'b13'), ('r1', 'b14'), ('r1', 'b15'),
    ('r1', 'b1081'), ('r1', 'b2'), ('r1', 'b3'), ('r1', 'b17'),
    ('r1', 'b18')
]

cb = ["r92", "r158", "r177", "r257", "r1042", "r206", "r333", "r1080", "r392",
      "r417", "r477", "r544", "r612", "r644", "r711", "r750", "r826", "r857",
      "r883", "r1010", "r1013", "r1014", "r899"]

URL_TEMPLATE = (
    "https://www.topcv.vn/tim-viec-lam-sales-xuat-nhap-khau-logistics-"
    "c{r}c{b}?type_keyword=1&page={page_number}&category_family={r}~{b}&sba=1"
)

SCROLL_END = 13000
SCROLL_STEP = 20
CLICK_INTERVAL = 0.8
CLICK_POSITION = (620, 337)
FIRST_CLICK_POSITION = (300, 300)


def save_to_excel(new_data, file_name):
    new_df = pd.DataFrame(new_data)

    if os.path.exists(file_name):
        # ƒê·ªçc d·ªØ li·ªáu c≈©
        old_df = pd.read_excel(file_name)

        # G·ªôp d·ªØ li·ªáu c≈© + m·ªõi
        combined_df = pd.concat([old_df, new_df], ignore_index=True)

        # X√≥a c√°c d√≤ng tr√πng (n·∫øu c·∫ßn, d·ª±a theo c·ªôt url)
        combined_df.drop_duplicates(subset=["url"], inplace=True, ignore_index=True)
    else:
        combined_df = new_df

    # Ghi ƒë√® l·∫°i file Excel
    combined_df.to_excel(file_name, index=False)
    print(f"üíæ ƒê√£ ghi {len(new_df)} d√≤ng m·ªõi, t·ªïng c·ªông {len(combined_df)} d√≤ng trong {file_name}")


def save_to_json(new_data, filename="data.json"):
    # N·∫øu file ƒë√£ t·ªìn t·∫°i th√¨ ƒë·ªçc d·ªØ li·ªáu c≈©
    if os.path.exists(filename):
        with open(filename, "r", encoding="utf-8") as f:
            try:
                existing_data = json.load(f)
                if not isinstance(existing_data, list):
                    existing_data = [existing_data]
            except json.JSONDecodeError:
                existing_data = []
    else:
        existing_data = []

    # Th√™m d·ªØ li·ªáu m·ªõi v√†o
    if isinstance(new_data, list):
        existing_data.extend(new_data)
    else:
        existing_data.append(new_data)

    # Ghi l·∫°i to√†n b·ªô d·ªØ li·ªáu ra file
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(existing_data, f, ensure_ascii=False, indent=4)

    print(f"‚úÖ ƒê√£ l∆∞u {len(new_data)} m·ª•c v√†o {filename}")


# ========================
# C·∫•u h√¨nh tr√¨nh duy·ªát
# ========================
options = Options()
options.add_argument("--start-maximized")
driver = webdriver.Chrome(options=options)
driver.maximize_window()

# ========================
# Bi·∫øn l∆∞u d·ªØ li·ªáu to√†n b·ªô
# ========================
all_data = []


# ========================
# H√†m scroll + click + crawl
# ========================
def scroll_and_crawl(driver, actions):
    current_scroll = 0
    last_click_time = time.time()
    while current_scroll < SCROLL_END:
        driver.execute_script(f"window.scrollTo(0, {current_scroll});")
        time.sleep(0.05)
        current_scroll += SCROLL_STEP

        if time.time() - last_click_time >= CLICK_INTERVAL:
            # Click m√¥ ph·ªèng
            actions.move_by_offset(CLICK_POSITION[0], CLICK_POSITION[1]).click().perform()
            actions.move_by_offset(-CLICK_POSITION[0], -CLICK_POSITION[1])
            time.sleep(2)
            last_click_time = time.time()

            try:
                job_detail = driver.find_element(By.CSS_SELECTOR, "div.box-job-info")
                job_header_detail = driver.find_element(By.CSS_SELECTOR, "div.box-header")
                headers = job_header_detail.find_elements(By.CSS_SELECTOR, "div.box-item-header")
                title = job_header_detail.find_element(By.TAG_NAME, "h2").text.strip()

                salary, location, experience = "", "", ""
                for header in headers:
                    if salary == "":
                        salary = header.text.strip()
                    elif location == "":
                        location = header.text.strip()
                    elif experience == "":
                        experience = header.text.strip()

                description = job_detail.get_attribute("innerText").strip()
                all_data.append({
                    "Title": title,
                    "Salary": salary,
                    "Location": location,
                    "Experience": experience,
                    "Description": description
                })
                print(salary, location, experience, description)

            except Exception as e:
                print("‚ö†Ô∏è Kh√¥ng l·∫•y ƒë∆∞·ª£c box-view-job-detail:", e)
                continue
    save_to_json(all_data, "crawl_job_details_full.json")
    all_data.clear()  # X√≥a d·ªØ li·ªáu sau khi l∆∞u


# ========================
# Ch·∫°y v√≤ng l·∫∑p ch√≠nh
# ========================


for r, b in cr_cb:
    page_number = 1
    url = URL_TEMPLATE.format(r=r, b=b, page_number=page_number)
    print(f"\nüöÄ ƒêang x·ª≠ l√Ω c·∫∑p ({r}, {b}) - Trang ƒë·∫ßu: {url}")
    driver.get(url)
    time.sleep(2)

    # ------------------------
    # L·∫•y s·ªë trang t·ªëi ƒëa
    # ------------------------
    try:
        span_text = driver.find_element(By.CSS_SELECTOR, "span#job-listing-paginate-text").get_attribute("innerHTML")
        # D·∫°ng: "1 /&nbsp; 5" -> l·∫•y s·ªë sau d·∫•u /
        match = re.search(r'/&nbsp;\s*(\d+)', span_text)
        max_page = int(match.group(1)) if match else 200
        print(f"üî¢ S·ªë trang t·ªëi ƒëa: {max_page}")
    except Exception:
        max_page = 200
        print("‚ö†Ô∏è Kh√¥ng t√¨m th·∫•y s·ªë trang, m·∫∑c ƒë·ªãnh 200")

    actions = ActionChains(driver)

    # ------------------------
    # Duy·ªát qua t·ª´ng trang
    # ------------------------
    for page_number in range(1, max_page + 1):
        try:
            url = URL_TEMPLATE.format(r=r, b=b, page_number=page_number)
            print(f"\n‚û°Ô∏è ƒêang v√†o trang {page_number}/{max_page} ({r},{b})")
            driver.get(url)
            time.sleep(2)
            scroll_and_crawl(driver, actions)
        except Exception as e:
            print(f"‚ùå L·ªói khi x·ª≠ l√Ω trang {page_number}: {e}")
            continue

# ========================
# Sau khi ho√†n t·∫•t
# ========================
driver.quit()